{"title": "Using Artificial Intelligence for Automatic Segmentation of CT Lung Images in Acute Respiratory Distress Syndrome", "pubDate": "2021", "PMCID": "PMC8476971", "DOI": "10.3389/fphys.2021.676118", "PMID": "34594233", "abstract": "Knowledge of gas volume, tissue mass and recruitability measured by the quantitative CT scan analysis (CT-qa) is important when setting the mechanical ventilation in acute respiratory distress syndrome (ARDS). Yet, the manual segmentation of the lung requires a considerable workload. Our goal was to provide an automatic, clinically applicable and reliable lung segmentation procedure. Therefore, a convolutional neural network (CNN) was used to train an artificial intelligence (AI) algorithm on 15 healthy subjects (1,302 slices), 100 ARDS patients (12,279 slices), and 20 COVID-19 (1,817 slices). Eighty percent of this populations was used for training, 20% for testing. The AI and manual segmentation at slice level were compared by intersection over union (IoU). The CT-qa variables were compared by regression and Bland Altman analysis. The AI-segmentation of a single patient required 5-10 s vs. 1-2 h of the manual. At slice level, the algorithm showed on the test set an IOU across all CT slices of 91.3 \u00b1 10.0, 85.2 \u00b1 13.9, and 84.7 \u00b1 14.0%, and across all lung volumes of 96.3 \u00b1 0.6, 88.9 \u00b1 3.1, and 86.3 \u00b1 6.5% for normal lungs, ARDS and COVID-19, respectively, with a U-shape in the performance: better in the lung middle region, worse at the apex and base. At patient level, on the test set, the total lung volume measured by AI and manual segmentation had a R 2 of 0.99 and a bias -9.8 ml [CI: +56.0/-75.7 ml]. The recruitability measured with manual and AI-segmentation, as change in non-aerated tissue fraction had a bias of +0.3% [CI: +6.2/-5.5%] and -0.5% [CI: +2.3/-3.3%] expressed as change in well-aerated tissue fraction. The AI-powered lung segmentation provided fast and clinically reliable results. It is able to segment the lungs of seriously ill ARDS patients fully automatically.", "author": [{"author": "Peter Herrmann", "affiliation": ["Department of Anesthesiology, University Medical Center G\u00f6ttingen, G\u00f6ttingen, Germany."], "href": "/?term=Herrmann+P&cauthor_id=34594233"}, {"author": "Mattia Busana", "affiliation": ["Department of Anesthesiology, University Medical Center G\u00f6ttingen, G\u00f6ttingen, Germany."], "href": "/?term=Busana+M&cauthor_id=34594233"}, {"author": "Massimo Cressoni", "affiliation": ["Unit of Radiology, IRCCS Policlinico San Donato, Milan, Italy."], "href": "/?term=Cressoni+M&cauthor_id=34594233"}, {"author": "Joachim Lotz", "affiliation": ["Institute for Diagnostic and Interventional Radiology, University Medical Center G\u00f6ttingen, G\u00f6ttingen, Germany."], "href": "/?term=Lotz+J&cauthor_id=34594233"}, {"author": "Onnen Moerer", "affiliation": ["Department of Anesthesiology, University Medical Center G\u00f6ttingen, G\u00f6ttingen, Germany."], "href": "/?term=Moerer+O&cauthor_id=34594233"}, {"author": "Leif Saager", "affiliation": ["Department of Anesthesiology, University Medical Center G\u00f6ttingen, G\u00f6ttingen, Germany."], "href": "/?term=Saager+L&cauthor_id=34594233"}, {"author": "Konrad Meissner", "affiliation": ["Department of Anesthesiology, University Medical Center G\u00f6ttingen, G\u00f6ttingen, Germany."], "href": "/?term=Meissner+K&cauthor_id=34594233"}, {"author": "Michael Quintel", "affiliation": ["Department of Anesthesiology, University Medical Center G\u00f6ttingen, G\u00f6ttingen, Germany.", "Department of Anesthesiology, DONAUISAR Klinikum Deggendorf, Deggendorf, Germany."], "href": "/?term=Quintel+M&cauthor_id=34594233"}, {"author": "Luciano Gattinoni", "affiliation": ["Department of Anesthesiology, University Medical Center G\u00f6ttingen, G\u00f6ttingen, Germany."], "href": "/?term=Gattinoni+L&cauthor_id=34594233"}], "refPMID": ["34071263", "32164153", "30944843", "22797452", "28060704", "8468771", "34061750", "29131760", "31811431", "34058628", "33089348", "17384569", "18451319", "23385101", "23348974", "23706034", "31601480", "30810231", "16641394", "1986640", "15812622", "3307572", "3298678", "31760194", "33446781", "34047928", "34058769", "16764513", "32814998", "32277135", "32046685", "33401581", "31316369", "34032344", "28759880", "33498999", "26017442", "33655564", "28778026", "31152273", "8111603", "9476848", "34067937", "33557772", "33273436", "32418337", "31634827", "34086716", "28689314", "34087705", "33993015", "34043732", "29498017", "34089126", "33362345", "32613207"], "citedInPMID": ["34594233", "34834455", "34829472"], "body": " AbstractKnowledge of gas volume, tissue mass and recruitability measured by the quantitative CT scan analysis (CT-qa) is important when setting the mechanical ventilation in acute respiratory distress syndrome (ARDS). Yet, the manual segmentation of the lung requires a considerable workload. Our goal was to provide an automatic, clinically applicable and reliable lung segmentation procedure. Therefore, a convolutional neural network (CNN) was used to train an artificial intelligence (AI) algorithm on 15 healthy subjects (1,302 slices), 100 ARDS patients (12,279 slices), and 20 COVID-19 (1,817 slices). Eighty percent of this populations was used for training, 20% for testing. The AI and manual segmentation at slice level were compared by intersection over union (IoU). The CT-qa variables were compared by regression and Bland Altman analysis. The AI-segmentation of a single patient required 5\u201310 s vs. 1\u20132 h of the manual. At slice level, the algorithm showed on the test set an IOU across all CT slices of 91.3 \u00b1 10.0, 85.2 \u00b1 13.9, and 84.7 \u00b1 14.0%, and across all lung volumes of 96.3 \u00b1 0.6, 88.9 \u00b1 3.1, and 86.3 \u00b1 6.5% for normal lungs, ARDS and COVID-19, respectively, with a U-shape in the performance: better in the lung middle region, worse at the apex and base. At patient level, on the test set, the total lung volume measured by AI and manual segmentation had a R2 of 0.99 and a bias \u22129.8 ml [CI: +56.0/\u221275.7 ml]. The recruitability measured with manual and AI-segmentation, as change in non-aerated tissue fraction had a bias of +0.3% [CI: +6.2/\u22125.5%] and \u22120.5% [CI: +2.3/\u22123.3%] expressed as change in well-aerated tissue fraction. The AI-powered lung segmentation provided fast and clinically reliable results. It is able to segment the lungs of seriously ill ARDS patients fully automatically.Keywords: ARDS, fully automatic lung segmentation, deep learning, U-Net, LabVIEW, DeepLTK, Maluna, mechanical ventilation IntroductionThe quantitative analysis of lung tomography [quantitative CT scan analysis (CT-qa)] images has been used extensively for more than 20 years and has significantly improved our knowledge of the pathophysiology of the acute respiratory distress syndrome (ARDS; ARDS Definition Task Force et al., 2012). Indeed, with CT-qa we have clarified how densities are distributed in ARDS, advancing the concept of the \u201cbaby lung\u201d (Gattinoni et al., 1987; Bone, 1993; Gattinoni and Pesenti, 2005), showing how densities redistribute in prone position (Gattinoni et al., 1991; Pelosi et al., 1998; Cornejo et al., 2013), and explaining the mechanisms by which positive end-expiratory pressure (PEEP) acts (Pelosi et al., 1994). Determining the change in the non-aerated tissue fraction at two end-expiratory pressure levels, i.e., 5 and 45 cmH2O, is considered the gold standard for assessing recruitment in ARDS (Gattinoni et al., 2006). We were recently able to show that CT-qa can also provide valuable information for the respiratory management of COVID-19 (Chiumello et al., 2020). A precise segmentation [the inclusion of a structure into a region of interest (ROI) for the subsequent analysis] of the lung is mandatory for a reliable CT-qa. The actual segmentation procedure in several hospitals required constisten manual intervention. The time requirement and need of expert personal has serious hindered a broder adoption of CT-qa in clinical practice.The application of machine learning techniques to image processing is currently of rapidly growing interest in the medical community (Seo et al., 2020a). Artificial neural networks (ANN) are a subfield of machine learning in which the underlying mathematical algorithm simulates the organization of the brain. By doing so, increasingly complicated tasks, such as voice and face recognition, recommender systems etc., which, until recently were considered impossible for a machine, have become part of our everyday life. The term deep learning (DL) goes back to around 2006 (Hinton et al., 2006). LeCun et al. (2015) explained DL in detail in 2015. Goodfellow et al. (2018) published an excellent textbook on DL in 2018. DL uses ANNs with many hidden layers. The larger the amount of data, the better DL works. Certain neural Network architectures such as the so-called convolutional neural networks (CNNs) are used specifically for image recognition. DL with CNNs are very common in medicine today (Chartrand et al., 2017; Litjens et al., 2017; Suzuki, 2017; Yasaka et al., 2018; Currie et al., 2019; Chassagnon et al., 2020; Al-Fatlawi et al., 2021; Guimar\u00e3es et al., 2021; J\u00fcnger et al., 2021; Schwartz et al., 2021; Su\u0142ot et al., 2021; Wang C. et al., 2021; Yi et al., 2021).There are two interesting CNN architectures for image segmentation. The \u201cSegNet\u201d developed by Badrinarayanan et al. (2017) and the U-Net developed by Ronneberger et al. (2015). SegNet was developed as an efficient architecture for semantic pixel-wise segmentation. It is primarily developed to recognize and classify in street scenes, streets, sidewalks, buildings, cars and pedestrians. SegNet was already used for Medical Image Segmentation (Sravani, 2019; Almotairi et al., 2020; Hu et al., 2020; Lei et al., 2021). The U-Net, was mainly developed for segmenting neuronal structures in electron microscopic stacks and light microscopic cell and tissue sections and works very effectively with comparatively little training data. In recent years, U-Net has been used successfully in medicine to segment certain structures and organs in chest x-rays and CT images (Zhou et al., 2018; Alom et al., 2019; Dong et al., 2019; Jeong et al., 2019; Hojin et al., 2020; Seo et al., 2020b; Umapathy et al., 2020; Causey et al., 2021; Ghosh et al., 2021; Wang Z. et al., 2021; Yan and Zhang, 2021). Some working groups have also segmented lungs in the CT images (Skourt et al., 2018; Park et al., 2020; Zhou et al., 2020; Chen et al., 2021; Jalali et al., 2021; Kumar Singh et al., 2021; Qiblawey et al., 2021). In a current publication, in the course of the COVID-19 pandemic, lung CT image segmentation with SegNet and U-Net was compared with each other, whereby the lungs segmented better with U-Net (Saood and Hatem, 2021). Gerard et al. (2020, 2021) developed a multi-resolution 3D-SegNet-CNN for the segmentation of inflamed, fibrotic and also ARDS lungs from the CT. This very interesting model consists of a high-res and a low-res Network and showed very good results. Seg3DNet is a fully convolutional CNN, but it uses less memory than SegNet and U-Net and can therefore process 3D images. There are now several modifications of the U-Net, such as U-Net ++ (Zhou et al., 2018), Res-U-Net (Umapathy et al., 2020), Recurrent Res-U-Net (Alom et al., 2019), 3D U-Net (Park et al., 2020), and more. Hofmanninger et al. (2020) trained a U-Net with 231 clinical cases (231 volumes with 108,248 slices). This data set contained different pathologies, reconstruction kernels, slice thicknesses, etc. This 2D U-Net231 showed surprisingly good results in the lung series tested. Hofmanninger et al. were able to show that automatic lung segmentation in routine clinical imaging is primarily a problem of data diversity. This was a very interesting aspect for us because we are working with clinical data. Due to the complexity of Networks such as 3DSegNet or the U-Net modifications, we decided to implement the 2D-U-Net.The successful application of DL to the segmentation process of CT lung images in ARDS would greatly increase the use of CT-qa. It would become available to clinical practice for monitoring relevant variables, such as the size of the lung, the severity of lung injury, hyperinflation, recruitability, differentiate atelectatic and consolidated tissue, and assess parenchymal homogeneity. We developed a DL algorithm, based on the graphical programming language LabVIEW to automatically and efficiently analyze and segment acutely injured lungs over the full spectrum of ARDS severity. Materials and EquipmentDataset DescriptionsThe CT scan dataset used in this study (n patients = 100, n slices = 12,279) was extracted from an ARDS dataset in which we included the patients enrolled into different trials or physiological studies from 2003 to 2018 the Policlinico Hospital in Milan. The CTs of these ARDS patients were taken during an end inspiratory pause at 45 cm of water (recruitment) and at 5 and 15 cm of water during end expiratory pressure. The CT scan of these patients were performed within 4.1 \u00b1 2.6 days after admission into the hospital. To this ARDS group, we added acquired CT scans from 20 COVID-19 patients (n slices = 1,817) from the San Paolo hospital, Milan and CT scans from 15 patients with normal lungs (n slices = 1,302) from the Medical University of G\u00f6ttingen. From the included patients, we obtained 15,398 CT slices which were all manually segmented. We did not exclude any lung slice. Computed Tomography scans from eighty percent of the patients, randomly selected (n patients = 108, n slices = 11,932) were used for training of the algorithm and the remaining 20% (n patients = 27, n slices = 3,466) for the testing dataset. The characteristics of the dataset used in this study are summarized in Figure 1 and the characteristics of the patient population are presented in Table 1. The technical characteristics of the CT used to acquire the images are reported in Table 2. The ethics committee was notified and permission to use the data was granted (G\u00f6ttingen Application Number 14/12/12).Open in a separate windowFIGURE 1Flowchart of the inclusion of patients and related CT scans. We collected data from different centers, from healthy and acute respiratory distress syndrome (ARDS) lungs. As shown, the model was also trained and tested with recently acquired COVID-19 lungs.TABLE 1Summary of the datasets used to train and test the convolutional neural network.Train datasetTest datasetSum\n\n\n\n\n\nPatients (n\u00b0)Slices (n\u00b0)Patients (n\u00b0)Slices (n\u00b0)Patients (n\u00b0)Slices (n\u00b0)Normal lung87167586151,302ARDS8910,222112,05710012,279COVID-19119949823201,817Sum10811,932273,46613515,398Open in a separate windowTABLE 2Technical characteristics of the CT scanner used.HospitalSan Paolo Hospital MilanPoliclinico Hospital MilanUniversity Hospital G\u00f6ttingenCT ScannerGE Light Speed Qx/iSiemens Somaton Definiton FlashGE Lightspeed VCTSiemens Sensation 16KVP120 kV, 140 kV120 kV, 140 kV140 kV120 kV, 140 kVSlice Thickness2.0, 2.5, and 5.0 mm5.0 mm5.0 mm5.0 mmPixel Spacing0.6\u20130.8 mm0.6\u20130.7 mm0.6\u20130.7 mm0.6\u20130.7 mmConvolution KernelSTANDARDB30f, B31f, B40fLUNGB41f, B75fFilter TypeBODY FILTER0, FLAT, WEDGE_3BODY FILTER0Patient PositionFFS, HFSFFSFFSFFS, HFSOpen in a separate windowHardware and Software UsedWe used a DELL Precision 5820 Tower with 32 GB RAM, a 3.70 GHz Intel (R) Xeon (R) W-2135 CPU and Windows 10 64-bit operating system. An Nvidia Quadro P 5000 with 16 GB GDDR5 RAM and 2560 CUDA cores was used as the graphics card. We used the same hardware for segmentation, quantitative analysis, and training. The U-Net was programmed with LabVIEW, NI-Vision (NI, Austin, TX, United States) and the Add-On Toolkit DeepLTK (Ngene, Yerevan, Armenia). There are many frameworks for DL (Caffe, Keras, TensorFlow, Theano, and Torch) on the market, but they mainly support the Python and C/C++ programming language. The DL toolkit, developed by Ngene, is a high abstraction level API providing the possibility to build, configure, train, evaluate and deploy deep neural Networks in the LabVIEW programming environment (Ngene, 2021). The GPU acceleration functionality of the toolkit is based on Nvidia\u2019s CUDA and CUDNN toolkit, by calling corresponding shared libraries. CUDA is a parallel computing platform and programming model using a GPU for general purpose computing, and CUDNN is a GPU-accelerated library of primitives for deep neural Networks. MethodsImage PreprocessingAnonymized CT scans of the lungs obtained in the Policlinico Hospital Milan, San Paolo Hospital Milan and the University Hospital G\u00f6ttingen were stored in DICOM (Digital Imaging and COmmunication in Medicine) format (\u2217.dcm) on DVD data carriers. In addition, a corresponding file with the coordinates of the manually drawn ROI was saved for each DICOM file (\u2217.xroi). These \u2217.xroi files were created as follows. All reference segmentations (ground truth) were carried out manually and/or semi-automatically by experienced intensive care physicians using our own software (Maluna 3.14, Maluna 2020). The coordinates of these lung masks were then saved for each CT in a so-called \u2217.xroi file (same file name as the Dicom image file name). They are loaded automatically when the DICOM image is loaded and placed as an ROI over the original image. Lung-specific calculations can be carried out within this ROI. Before the ANN could be trained with the lung CTs, the original DICOM images were preprocessed (Figure 2). In a first step, the gray values were converted into Hounsfield units (HUs) from the original 16-bit DICOM image (A,G) using the DICOM attributes \u201cRescale Intercept\u201d (\u00d70028, \u00d71,052) and \u201cRescale Slope\u201d (\u00d70028, \u00d71,053). The resulting image was then scaled to the range \u22121,024 HU to +100 HU and then converted into an 8-bit image (B,H). An 8-bit image is one with 256 levels of gray. A binary thorax mask was created (C, I) using a threshold and particle filter (Klapsing et al., 2017). Image B (H) was then masked in order to remove superfluous information from the image (D, J). In the final step, the pixels were normalized to the range between \u22121 and +1. This was done by subtracting 128 from each pixel and then dividing the result by 128.Open in a separate windowFIGURE 2Image preprocessing steps to create the input data for the artificial neural network. (A) Original 16-bit gray value Digital Imaging and COmmunication in Medicine (DICOM) image. (B) 16-bit HU image. (C) 8-bit image. (D) Binary thorax mask. (E) Masked image. (F) Normalized image. (G) Manually drawn ROI. (H) Binary mask created from the region of interest (ROI). (I) Normalized lung mask (ground truth). (K) Rotated prone image. (L) Rotated prone mask.To create the lung mask (known as \u201cground truth\u201d), the ROI coordinates loaded from the \u2217.xroi file drawn as an ROI in image E (K) are converted into a binary mask (F, L). Then, as with the lung image, the mask is normalized to the range \u22121 to +1 (I). For the CT images that were not obtained with the patient in the supine position, the images and masks were rotated accordingly (H, I M). To check the position of the patient, we used the DICOM attribute \u201cpatient position\u201d (\u00d70018, \u00d75,100). A total of 11,932 images and their manually generated ROI coordinates were preprocessed in this way and then loaded into the ANN. The image preprocessing was carried out with our own software Maluna 2020.The ANNStructure of the ANN The network we use is based on the U-Net architecture. The U-Net was programmed with the graphical programming language LabVIEW, with which we had many years of experience in the development of software for image analysis. The unique concept of U-Net is that it is able to generate a new, altered image as the output from an input image, after appropriate processing. This is very useful for generating segmentation images. The U-Net is a so-called fully convolutional network. Our U-Net programmed with LabVIEW is shown in Figure 3.Open in a separate windowFIGURE 3Our used U-Net architecture. Each green or blue box corresponds to a multi-channel feature map. The number of channels is shown above the box. The specifications 512 \u00d7 512 to 16 \u00d7 16 (in the lowest resolution) show the x, y dimensions in pixels of the input and output images (or feature maps).The architecture has a symmetric \u201cU\u201d shape and consists of two major parts: a contraction path (left side) and an expansion path (right side). The path follows the typical architecture of a convolution neural network. It consists of the repeated application of two convolution layers, each layer with batch normalization, followed by an activation function. In all convolution layers we use a filter kernel size of 3 \u00d7 3 pixels. For each convolution we used the so called \u201cSAME\u201d padding type, which means there is automatically enough padding that the output image of the convolution layer has the same dimensions as the input image.In the original U-Net by Ronneberger, the image is filtered twice with 64 convolution filters in the first level of the contraction path. Due to insufficient graphics memory, we had to modify the original U-Net a bit. In the first level of the contraction path, the preprocessed lung CT image is therefore filtered twice with only 32 different convolution kernels. We used a filter kernel size of 3 \u00d7 3 pixels. A copy of this batch of 32 filtered images is transferred to the right part of the network. In the original U-Net, the next step is a max pooling layer for down sampling. We used a 3 \u00d7 3 convolution layer with stride 2, which halves the size of the input image (from 512 \u00d7 512 to 256 \u00d7 256 pixels) and doubled in the number of filtered images (from 32 to 64 images or channels). This principle, i.e., twice convolution filtering, halving the image size, and doubling the number of channels, is followed until we finally get a stack of 1,024 channels with a size of 16 \u00d7 16 pixels (this size is approximately in the range of an acinus). Since these small images no longer have any resemblance to the original image, but show certain extracted properties of the image, i.e., corners, edges, structures, they are also referred to as feature maps. These feature maps are processed further in the expansion path (right part) of the U-Net.Each step on the expansion path consists of upsampling the feature map, followed by a convolution layer (\u201cup-convolution\u201d) which halves the number of feature channels and doubles the size of the input image. Then, a concatenation is carried out with the corresponding feature map from the contraction path (left part of the U-Net) followed by two convolution layers with batch normalization and an activation function. In the lowest path with the lowest resolution (16 \u00d7 16 pixels, 1,024 feature channels) a drop-out layer (with probability set to 0.4) was programmed between two convolutional layers.Preprocessed lung CT-images are input to the contracting path, and lung mask predictions are output from a final layer following the expansive path. This final output layer is a 1 \u00d7 1 convolutional layer with no activation and a single filter. Batch normalization and dropout are proven methods of avoiding overfitting with CNNs (Srivastava et al., 2014; Ioffe and Szegedy, 2015).Convolution Layers The input image is first processed by a set number of convolution filters that have a fixed pixel size. In our case the size was 3 \u00d7 3 pixels. This filter then moves in a constant step size (stride) like a window from left to right over the pixels of the input image. After each pass, the filter skips to the next-lower row. The so-called padding is used to determine how the filter should behave when it hits the edge of the matrix. We use \u201cSAME\u201d padding. With \u201cSAME\u201d padding and Stride 1, the convolution layer output will have the same spatial dimensions as its input. With a 3 \u00d7 3 pixel filter, nine pixels of the input image are simultaneously connected to the filter (local connectivity) and are convolved to a new value.The following equation shows the computation of the discrete convolution\nO\u2062[i,j]=\u2211p=-S2S2\u2211q=-S2S2[i-p,j-q]\u22c5K\u2062[p,q]\n(1)\nO=I\u22c5K\u2062(c\u2062o\u2062n\u2062v\u2062o\u2062l\u2062u\u2062t\u2062i\u2062o\u2062n)\n(2)where I is the Input Image, O is the Output Image, K is the Filter Kernel, and S is the Filter Size.Depending on the property and number of filters, the convolution layer is able to recognize and extract individual features in the input data. These can be lines, edges or certain shapes (Figure 4). The step size of the filter determines whether the output image should have the same size as the input image, or whether it should be reduced in size. For example, for downsampling we chose a stride of 2 to halve the size of the input image. For upsampling we use an upsampling layer. This layer increases the dimensionality (rows and columns) of output feature maps by doubling the values (stride = 2).Open in a separate windowFIGURE 4Input image and five different output images generated with different convolution kernels.Activation Function In the activation function of the neural network, you decide whether the neuron fires or not. There are different types of activation functions such as sigmoid function, tangent function, rectified linear unit (ReLU) and leaky rectified linear unit (LReLU). In our case we used LReLU. LReLU s are one attempt to fix the \u201cdying ReLU\u201d problem. Instead of the function being zero when x < 0, a leaky ReLU will instead have a small negative slope like 0.1 or 0.3 (Maas et al., 2013; Goodfellow et al., 2018). That is, the function computes:\nf(x)=1(x<0)\u22c5(\u03b1\u22c5x)+1(x\u22650)\u22c5(x)\n(3)where \u03b1 is a small constant. So, if the input x is greater than 0, then the output is x. If the input is less than 0, the output will be alpha \u03b1 times the input.In the DeepLTK toolkit LReLU activation function uses 0.1 as a hardcoded alpha parameter.Batch Normalization Batch normalization is a layer that allows every layer of the network to perform learning more independently. Batch normalization can be used as a regularization strategy to avoid overfitting the model. The layer is added to the sequential model to standardize the input or the outputs. It can be used at several points between the layers of the model. It is often inserted just after defining the sequential model and after the convolution and pooling layers. Batch normalization is a technique that has been widely used over the years and has proven to be very effective in several DL tasks. It uses the mean and variance computed within a small data stack to normalize its features during activation (Ioffe and Szegedy, 2015).Dropout Layer Dropouts are the regularization technique that is used to prevent overfitting in the model. Dropouts are added to randomly switching some percentage of neurons of the network. When the neurons are switched off, the incoming and outgoing connections to those neurons are also switched off. This prevents units from co-adapting too much (Srivastava et al., 2014).Initialization of the Weights in the ANN With each pass through a layer, the variance should remain as constant as possible. This prevents the signal from increasing toward infinity or vanishing to zero. This means that the weights in the network must be initialized so that the variance for x and y remains the same. This initialization process is known as Xavier initialization (Glorot and Bengio, 2010). We use Xavier initialization for all the weights in our U-Net.Training of the U-NetThe ANN programmed in this manner was trained with 11,932 CT slice images of lungs and the associated manually drawn lung masks (ground truth). The training was performed on 113,784 iterations. One iteration includes miniBatch sampling (we use a miniBatch size of 12) \u2192 Forward Propagation \u2192 Loss Evaluation (the predicted masks were compared to the manually generated lung masks) \u2192 Back Propagation and update of the weights in the network. In simple terms the network then tried to minimize the error between the manual mask and the mask generated in each iteration by selecting the appropriate combinations of convolution filters with more than 6,000 different convolution kernels used. This took about 1.4 s. The complete training duration was 44.2 h. Return values are the evaluated loss value and a value for the current iteration. We used the Mean Square Error Regression Loss function. The course of the learning curve is shown in Figure 5. The complete training process is shown in Figure 6.Open in a separate windowFIGURE 5Graphical representation of the loss during the training period over 44.2 h.Open in a separate windowFIGURE 6The complete training workflow.Testing of the Trained ModelThe trained U-Net was tested on 3,466 CT lung slice images from 27 patients. The predicted ROIs were used to segment each slice and the whole lung CT-qa was then performed. Briefly, the lung is composed by two compartments with very different densities: tissue, with a density close to the one of water (0 HUs), and gas, with a density of \u22121,000 HU. For each voxel:\nVg\u2062a\u2062s=-C\u2062T\u2062(H\u2062U)1,000\u22c5Vv\u2062o\u2062x\u2062e\u2062l\n(4)\n\u03c1L=C\u2062T+1,0001,000\n(5)\nT\u2062i\u2062s\u2062s\u2062u\u2062e\u2062m\u2062a\u2062s\u2062s=\u03c1L\u22c5Vv\u2062o\u2062x\u2062e\u2062l\n(6)The voxel gas volume and voxel tissue mass were multiplied by the total number of voxels to obtain the total tissue mass and the total gas volume. Lung tissue was classified according to its gas/tissue content as not inflated (CT number between +100 and \u2212100), poorly aerated (CT number between \u2212101 and \u2212500), normally inflated (CT number between \u2212501 and \u2212900), and hyper-inflated (CT number between \u2212901 and \u22121,000) (Cressoni et al., 2013).We estimated recruitability as:\nR\u2062e\u2062c\u2062r\u2062u\u2062i\u2062t\u2062a\u2062b\u2062i\u2062l\u2062i\u2062t\u2062y=n\u2062o\u2062n\u2062a\u2062e\u2062r\u2062a\u2062t\u2062e\u2062d\u2062t\u2062i\u2062s\u2062s\u2062u\u2062e5\u2062c\u2062m\u2062H2\u2062O-n\u2062o\u2062n\u2062a\u2062e\u2062r\u2062a\u2062t\u2062e\u2062d\u2062t\u2062i\u2062s\u2062s\u2062u\u2062e45\u2062c\u2062m\u2062H2\u2062On\u2062o\u2062n\u2062a\u2062e\u2062r\u2062a\u2062t\u2062e\u2062d\u2062t\u2062i\u2062s\u2062s\u2062u\u2062e5\u2062c\u2062m\u2062H2\u2062O\n(7)The first formula indicates the fraction of gasless tissue which regains inflation increasing the pressure. The complete testing workflow is shown in Figure 7.Open in a separate windowFIGURE 7The complete test and analyzing workflow.Statistical AnalysisThe masks obtained by manual and artificial segmentation were compared by the intersection over nion (IoU) metric method. The variables computed by CT quantitative analysis after manual and artificial Intelligence (AI)-segmentation were compared by linear regression, and Bland-Altman analysis, and calculating 95% confidence intervals to evaluate the agreement between the masks. Student\u2019s t test was used to test the difference between the means of normally distributed values. Otherwise, we used the Wilcoxon test. Two-tailed p values < 0.05 were considered statistically significant. All statistical analyses were performed using R 4.1 (The R Project for Statistical Computing).Intersection Over Union Metric The IoU, also known as the Jaccard index, is an established method for determining the segmentation quality of segmented images. It is used to quantify the correspondence between the manually created lung mask (ground truth) and the lung mask predicted by the trained model. The IoU metric measures the number of pixels common to the manually created masks and the prediction masks divided by the total number of pixels present in both masks. A value of 1 indicates a 100% agreement of the masks and 0 means no agreement (Nowozin, 2014).The following equation shows the computation of the IoU:\nI\u2062o\u2062U=|A\u2229B||A\u222aB|\n(8)where A is the manual generated mask (ground truth) and B is the predicted mask. ResultsSlices-Level PerformanceFor all lung scans, the agreement between manual and AI-segmentation (IoU metric) was 87% \u00b1 10% in the test set, as shown in Table 3. In Figure 8 we show the agreement between manually and AI-segmentation in the training and test sets along the cranio-caudal axis in normal lungs (Panel A), ARDS (Panel B), and COVID-19 (Panel C). Regardless of the lung type, the mean agreement between manual and AI segmentation across all CT slices was 91.3 \u00b1 10.0, 85.2 \u00b1 13.9, and 84.7 \u00b1 14.0%, and across all lung volumes 96.3 \u00b1 0.6, 88.9 \u00b1 3.1, and 86.3 \u00b1 6.5% for normal lungs, ARDS and COVID-19, respectively. In this test set, we found that the agreement between manual and AI-segmented lungs followed an inverse U-shape: higher in the central regions of the thorax and lower at the apex or near in the pleural recesses. Note that in these regions, the absolute amount of lung tissue is just a small fraction (4.1 \u00b1 2.0%) of the entire parenchyma. The worst results were obtained in severe ARDS compared with moderate and mild ARDS (Figures 8D\u2013F). Figure 9 shows the worst segmentation results, mainly in the peripheral zones of the lung slices. In addition to the IOU metric, the difference in lung volume between ground truth and predicted mask can also be seen. Figure 10 shows the best segmentation results with up to 99% agreement (IOU = 99%) in normal lungs.TABLE 3Mean Intersection over Union calculated per slices and per volumes.IoUmean \u00b1 SD per slicesN SlicesIoUmean \u00b1 SD per volumesN volumesNormal lung91.3 \u00b1 10.1%58696.3 \u00b1 0.6%9ARDS85,2 \u00b1 13.9%2,05788.9 \u00b1 3.1%30COVID-1984,7 \u00b1 14.0%82386.3 \u00b1 6.5%12All lungs87.3 \u00b1 10.0%3,46689.6 \u00b1 5.1%51Open in a separate windowOpen in a separate windowFIGURE 8Intersection over Union (IoU) metric performance on the training (green line) and test set (blue line) along the cranio-caudal axis in normal lungs (A), ARDS (B), COVID-19 (C), severe ARDS (D), moderate ARDS (E), and mild ARDS (F). As shown, in the training set, the AI algorithm almost perfectly matched the manual segmentation. In the test set the performance was slightly poorer. The Figure also shows the anatomical distribution of the error. Indeed, the algorithm was able to achieve a higher performance in the middle of the lung, while at the apex and the base, locations where also for a trained eye is sometimes difficult to distinguish the lung parenchyma from the surrounding structures and the pleural effusion, it struggled more.Open in a separate windowFIGURE 9Selected slices with very poor segmentation results with an IOU of up to 43% in ARDS Lung. Most of the lungs were poorly recognized in the peripheral areas. (A) preprocessed CT, (B) ground truth (transparent green), (C) predicted mask (transparent blue), (D) quantitative calculations within the manual ROI and (E) quantitative calculations within the automatic ROI (green = normally aerated, blue = overextended, orange = badly aerated and red = not aerated).Open in a separate windowFIGURE 10Selected slices with very good segmentation results with an IOU of up to 99% with normal lungs. The differences in lung volume between manually and automatically segmented lungs are 0.24 ml. (A) preprocessed CT, (B) ground truth (transparent green), (C) predicted mask (transparent blue), (D) quantitative calculations within the manual ROI and (E) quantitative calculations within the automatic ROI (green = normally aerated, blue = over-inflated, orange = poorly aerated and red = not aerated).Patient-Level PerformanceLung Volume The regressions and the Bland Altman analysis of the total lung volumes (gas + tissue volume) computed with manual and AI-segmentation both in the training set and in the test set are summarized in Supplementary Figure 6. As shown, the regression lines in these sets were close to identity. The Bland Altman plots on the sets showed biases of \u22123.1 ml [CI +13.0/\u221219.1] and \u22129.8 ml [CI: +56.0/\u221275.7 ml], respectively.Lung Tissues In the CT-qa, the overinflated, well-aerated, poorly aerated and non-aerated tissue fractions were almost identical in the manually or AI-segmentated images. Indeed, the R2 of the linear regressions between manual and AI-segmentation on overinflated, well-aerated and poorly aerated and non-aerated tissue was 0.99, 0.99, 0.98, and 0.91, respectively. The Bland Altman analyses comparing overinflated (p = 0.99), well-aerated (p = 0.91), poorly aerated (p = 0.91), and non-aerated tissues (p = 0.53) obtained after manual and AI-segmentation on the test set is summarized in Figure 11. The whole lung tissue fraction in all cases is 41.8 \u00b1 17.2% in manual segmentation and 41.1 \u00b1 16.2% (p = 0.85).Open in a separate windowFIGURE 11Bland Altman analysis of the agreement between manual and AI-segmentation on the test set when CT-qa was used to measure the over-inflated (A), well-aerated (B), poorly inflated (C) and non-aerated (D) tissues on the test set. As shown, biases never exceeded 50 g. The largest CI were, as expected, in the non-aerated tissue, where also for a trained eye is, at times, difficult to distinguish parenchyma from pleural effusion.Recruitability Assessment of recruitability is likely the most relevant variable that can be measured with CT-qa. In Supplementary Figure 7, we report the recruitment fraction computed for the manual and AI-segmented lungs in the test set. The recruitment is expressed both as variations of non-aerated tissue (panel A) and as a variation of well-aerated tissue (panel B). The agreement between the two techniques is within +6.2 and \u22125.5% (bias +0.3%) when the recruitment is expressed as variation of the percentage of non-aerated tissue and between +2.3 and \u22123.3% (bias \u22120.5) when expressed as variation of the percentage of well-aerated tissue.Inaccuracies To determine the inaccuracies of manual and AI-segmentation we assumed that the lung weight should not change in the same individual when increasing the airway pressure from 5 to 15 and to 45 cmH2O. A difference in lung weight between the two airway pressure levels can be considered as a sign of segmentation inaccuracy. As shown in Table 4, the average lung weight differences between 5 and 15 cmH2O or between 5 and 45 cmH2O obtained by manual and AI-segmentation were negligible. However, in the individual patients the differences could be as high as 336 g.TABLE 4Differences in lung weight detected at different airway pressures.Train datasetTest dataset\n\n\n\nManualAutomaticManualAutomatic5\u201315 cmH2OMean (g)\u20136.5\u00a0\u00a01.25\u201315 cmH2OMean (g)17.2\u201314.8\u00a0\u00a0SD (g)64.760.5SD (g)83.954.3Min (g)\u2013227.5\u00a0\u00a0\u00a0\u00a0\u2013168.5\u00a0\u00a0\u00a0\u00a0Min (g)\u2013103.0\u00a0\u00a0\u00a0\u00a0\u2013150.4\u00a0\u00a0\u00a0\u00a0Max (g)173.2\u00a0\u00a0196.2\u00a0\u00a0Max (g)194.8\u00a0\u00a042.65\u201345 cmH2OMean\u00a0\u00a07.514.55\u201345 cmH2OMean39.8\u20132.6SD (g)93.892.6SD (g)92.172.4Min (g)\u2013226.0\u00a0\u00a0\u00a0\u00a0\u2013231.9\u00a0\u00a0\u00a0\u00a0Min (g)\u201394.8\u00a0\u00a0\u2013145.7\u00a0\u00a0\u00a0\u00a0Max (g)336.0\u00a0\u00a0339.8\u00a0\u00a0Max (g)150.7\u00a0\u00a0101.8\u00a0\u00a0Open in a separate windowWorkload The complete training of the neural Network up to the level used in this analysis lasted 44.2 h. The learning curve of the algorithm is reported in Figure 5. With our current configuration the analysis of an unknown single CT slice requires 0.041 \u00b1 0.007 s. Therefore, automatic segmentation of a complete lung CT-scan with approximately 100 slices of 5.0 mm thickness, required approximately 5 s. DiscussionIn this study, we found that automatic lung segmentation performed by a properly trained neural network provided lung contours in close agreement with the ones obtained by manual segmentation. When comparing lung CT slices with the original Ronneberger network (Hofmanninger et al., 2020, Table 3: test data set for lung slice only), the IOU of damaged lungs is in a similar range (85% vs. 80\u201387% for trauma and 85% vs. 83\u201391% for atelectasis). In the case of normal lungs, however, the results are worse in comparison (91% vs. 94%). The automatic approach completed segmentation of the entire lung in 5\u201310 s making immediately available the CT-qa. Therefore, the whole process from DICOM image extraction to the lung CT-qa with data on the fractions of inflated, well aerated, poorly aerated and non-aerated tissue, as well as lung recruitability can be completed in just a few minutes. Beyond their use for research, these data may prove important for the clinical diagnosis and respiratory therapy. Indeed, the greatest limitation in implementing CT-qa in the everyday clinical practice is the amount of man-hours required for lung segmentation. This study presents a possible solution to this problem. The trained model is not perfect, as it showed weaknesses in the edge regions of the apex and base, especially in severely damaged lung areas that are difficult to identify even for a trained radiologist. These represent, however, only a minor fraction of the total lung parenchyma that this should not exceed 10% of the entire lung mass. Overall, the results obtained are fully adequate for pathophysiological decision processes and consequent clinical application.We may wonder to which extent one may be confident in the AI-segmentation compared to the manual one. In ARDS, image segmentation is especially difficult as, in some cases, it is almost impossible to discriminate the edge of the lung parenchyma from a pleural effusion, so common in ARDS (Chiumello et al., 2013), particularly in the most dependent lung regions and most severe ARDS forms. However, this problem is also present in manual segmentation. Indeed, when the CT scan of the same lung is taken under different operating conditions, for example, at different airway pressures, we observed, as in previous studies, differences in lung weights which, on average were rather trivial (\u223c10\u201320 g), but they could be as high as 336 g in the individual patient. These variations may result either from the segmentation procedure and/or from actual changes in lung weight, primarily due to a possible airway pressure-dependent blood shift. It is unfortunately impossible to determine how much of the weight variation is due to an intrathoracic blood shift or to inaccuracies of the segmentation process. The decrease in intrathoracic blood volume we estimated in a previous work (Chiumello et al., 2007) with increasing airway pressures was about 100 ml, leading to a small decrease in lung weight. In the present study, we found more pronounced variations of lung weight between 5 and 45 cmH2O than between 5 and 15 cmH2O. Indeed, especially in the train set, we found maximum differences in lung weight between the two pressure levels as large as 336 g, making unlikely that blood shift alone accounted for the entire variation. Indeed, at 5 cmH2O it is more difficult, even for trained personnel, to discriminate between parenchyma and pleural effusion, a process that is easier at 45 cmH2O. Therefore, manual segmentation is intrinsically associated with some degree of inaccuracy. Interestingly, as shown in Table 3, AI had the same degree of inaccuracy and closely mimicked the manual segmentation. Moreover, the more severe the ARDS, i.e., the extent of the densities and pleural effusions that are is present in approximately 80% of ARDS patients (Gattinoni et al., 1986), the greater the probability of inaccuracy. As opposed to the CT images of the training set, the lungs of the patients in the test CT were more poorly segmented, which may indicate a slight overfitting of the CNNs.The most relevant quantitative CT variables that may have an impact on clinical management are the recruitability and the volume of the lung open to the gas. The latter is frequently referred to as \u201cbaby lung,\u201d since in ARDS its size may resemble that of a 3-year-old child. The baby lung is represented by the amount of normally aerated tissue, which conventionally includes the voxels between \u2212500 and \u2212900 HUs (Gattinoni et al., 1986). AI-segmentation performed remarkably well under this definition, with an overall agreement within few grams. Knowledge of the baby lung and its associated gas volume will allow a straightforward measurement of the strain occurring during mechanical ventilation. Determining the strain, i.e., the ratio of tidal volume plus PEEP volume to the FRC, is a fundamental information when setting the ventilator, since excessive strain is a primary cause of ventilation-induced lung injury (Chiumello et al., 2008).The recruitability can be estimated either by assessing the amount of lung tissue which regains aeration, or by measuring the increase in the size of the baby lung when the airway pressure is increased. This allows the normal aeration of pulmonary units, which were previously collapsed or simply poorly inflated. Measuring recruitment as a non-aerated tissue fraction difference had a bias of +0.3% (CI: +6.2/\u22125.5%) on the test set. We believe, from a clinical standpoint, that these numbers are more than adequate to define the recruitability, which is usually roughly defined as a binary variable, i.e., the patient is either a \u201crecruiter\u201d or a \u201cnon-recruiter.\u201d A more precise definition of recruitability, which may range from 0% to more than 50% of the total lung mass, that would be easily clinically available with AI-segmentation, may represent an important step ahead when tailoring mechanical ventilation or setting PEEP. When we defined recruitment as changes in the baby lung dimensions, AI performed extremely well compared with manual segmentation.Most of the advances on our pathophysiological understanding of ARDS derive from the quantitative CT scan analysis. An easy availability of the CT-qa may play an important role in setting a proper ventilation and, maybe more importantly, to avoid harmful approaches. LimitationsIn the original U-Net, the original input image is first convoluted with 64 filters. Due to hardware limitations, this was not possible with our U-Net, so we started with 32 convolution filters. We have found that in the edge areas of the lungs, especially with very badly damaged lung tissue, segmentation is much worse. In addition, the lung CTs unknown to the model are segmented more poorly than the trained lung CTs, which indicates on the one hand that the Network is slightly overfitted, or on the other hand, that training was carried out with too few lung CT variations. Our ANN was developed with NI-LabVIEW, NI-Vision and the Deep Learning Toolkit from Ngene (DeepLTK). These are all commercial, license-protected software products. This means that an application cannot be used freely. The DeepLTK still has a number of limitations: IOU and Dice coefficient are the only metrics so far. More will be added in the next releases. Shape quality performance metrics like ASSD or BF-Score are not yet supported. The only optimization algorithm is Stochastic gradient descent. Further algorithms such as Adam, Adagrad, AdaDelta, RMSProp, and Nesterov are being developed for the next releases. So far, Mean Squared Error is the only loss type for 3D data. Cross Entropy loss is currently only available for 1D data. But it will be available for 3D data in the next update. It is a specific of DeepLTK toolkit (at the moment) that the complete dataset is preloaded on the CPU RAM (as a 4D single precision floating point array) to speed up miniBatch fetching and feeding to the Network for the training process. As during the training process, the whole dataset will be utilized for several (hundreds of) epochs, it is reasonable to preload decoded dataset and store it on the RAM to speed up the training process. Loading a miniBatch of data from disk is also reasonable, in case of large datasets, and at the cost of speed, but currently it is not implemented in the toolkit. A 3D semantic segmentation architecture is still not possible with the DeepLTK. ConclusionThe trained model based on the U-Net can automatically segment the lungs in the CT with the limitations mentioned. The automatic segmentation of a full lung CT scan with approximately 100 sections with a slice thickness of 5.0 mm took approximately 5 s, compared to manual segmentation which can take up to an hour. Due to the still poor performance compared to Python-based CNNs, we plan to further improve the U-Net developed with LabVIEW and optimize it for the detection of differently damaged lung areas. We are convinced that the widest possible variety of different lung pathologies and CT reconstruction parameters can significantly improve a suitable segmentation CNN. Therefore, we will increase the amount of input data through augmentation by varying the brightness, contrast, gamma and grain of the CT images and applying them to an increasing number and variety of lung pathologies. We plan to further modify the Network architecture through tests, while changing the miniBatch size, varying the probability of dropout layers, and varying the training parameters (i.e., optimizer, loss type, momentum, weight decay, and training type) are interventions for future research and further improvement. This should widen the field of potential applications and increase the already convincing validity of image data processing. In order to play with all these possibilities, one will require greatly advanced hardware with much better performance compared with the hardware used for this study.The development of a reliable clinical diagnostic system, able to perform the automatic detection and consecutively the quantitative analysis of lung tissues immediately after performance of a lung CT scan seems conceivable and also practicable. Such a tool would have significant impact on diagnosing and selecting the appropriate therapeutic interventions for each individual patient suffering from severe lung injury. Data Availability StatementThe raw data supporting the conclusions of this article will be made available by the authors, without undue reservation. Author ContributionsPH: study design, programming of the ANN, data collection, data analysis, figure design, and writing. MB: data analysis, statistical interpretation, figure design, and writing. MC: data collection, data interpretation, and image processing. LS, OM, KM, MQ, and LG: study design, data analysis, data interpretation, and writing. JL: data analysis, data interpretation, and revision of the manuscript. All authors critically revised and accepted the manuscript in its current form. Conflict of InterestThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. Publisher\u2019s NoteAll claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. AcknowledgmentsWe would like to thank Robert Ventzki and Silvio Rizzoli for the short-term loan of the DELL workstation, without which the training of the artificial neural network would not have been possible. We also thank Alik Sargsyan (Developer of the DeepLTK Toolkit and Founder & CEO of Ngene, Armenia) for the valuable tips on programming the first artificial neural Network with the DeepLTK Toolkit for LabVIEW. ReferencesAl-Fatlawi A., Malekian N., Garc\u00eda S., Henschel A., Kim I., Dahl A., et al.  (2021). Deep Learning Improves Pancreatic Cancer Diagnosis Using RNA-Based Variants.\nCancers\n13:2654. 10.3390/cancers13112654\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]Almotairi S., Kareem G., Aouf M., Almutairi B., Salem M. A. (2020). Liver Tumor Segmentation in CT Scans Using Modified SegNet.\nSensors\n20:1516. 10.3390/s20051516\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]Alom M. Z., Yakopcic C., Hasan M., Taha T. M., Asari V. K. (2019). Recurrent residual U-Net for medical image segmentation.\nJ. Med. Imaging\n6:014006. 10.1117/1.JMI.6.1.014006 [PMC free article] [PubMed] [CrossRef] [Google Scholar]ARDS Definition Task Force V. M., Rubenfeld G. D., Thompson B. T., Ferguson N. D., Caldwell E., et al.  (2012). Acute respiratory distress syndrome: the Berlin Definition.\nJAMA\n307\n2526\u20132533. [PubMed] [Google Scholar]Badrinarayanan V., Kendall A., Cipolla R. (2017). SegNet: a deep convolutional encoder-decoder architecture for image segmentation.\nIEEE Trans. Pattern Anal. Mach. Intell.\n39\n2481\u20132495. 10.1109/tpami.2016.2644615\n [PubMed] [CrossRef] [Google Scholar]Bone R. C. (1993). The ARDS lung. New insights from computed tomography.\nJAMA\n269\n2134\u20132135. 10.1001/jama.1993.03500160104042 [PubMed] [CrossRef] [Google Scholar]Causey J., Stubblefield J., Qualls J., Fowler J., Cai L., Walker K., et al.  (2021). An Ensemble of U-Net Models for Kidney Tumor Segmentation with CT images.\nIEEE/ACM Trans. Comput. Biol. Bioinform.\n10.1109/TCBB.2021.3085608\n[Epub ahead of print].  [PubMed] [CrossRef] [Google Scholar]Chartrand G., Cheng P. M., Vorontsov E., Drozdzal M., Turcotte S., Pal C. J., et al.  (2017). Deep Learning: a Primer for Radiologists.\nRadiographics\n37\n2113\u20132131. 10.1148/rg.2017170077\n [PubMed] [CrossRef] [Google Scholar]Chassagnon G., Vakalopolou M., Paragios N., Revel M. P. (2020). Deep learning: definition and perspectives for thoracic imaging.\nEur. Radiol.\n30\n2021\u20132030. 10.1007/s00330-019-06564-3\n [PubMed] [CrossRef] [Google Scholar]Chen K. B., Xuan Y., Lin A. J., Guo S. H. (2021). Lung computed tomography image segmentation based on U-Net Network fused with dilated convolution.\nComput. Methods Programs Biomed.\n18:106170. 10.1016/j.cmpb.2021.106170\n [PubMed] [CrossRef] [Google Scholar]Chiumello D., Busana M., Coppola S., Romitti F., Formenti P., Bonifazi M., et al.  (2020). Physiological and quantitative CT-scan characterization of COVID-19 and typical ARDS: a matched cohort study.\nIntensive Care Med.\n46\n2187\u20132196. 10.1007/s00134-020-06281-2\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]Chiumello D., Carlesso E., Aliverti A., Dellac\u00e0 R. L., Pedotti A., Pelosi P. P., et al.  (2007). Effects of volume shift on the pressure-volume curve of the respiratory system in ALI/ARDS patients.\nMinerva Anestesiol.\n73\n109\u2013118. [PubMed] [Google Scholar]Chiumello D., Carlesso E., Cadringher P., Caironi P., Valenza F., Polli F., et al.  (2008). Lung stress and strain during mechanical ventilation for acute respiratory distress syndrome.\nAm. J. Respir. Crit. Care Med.\n178\n346\u2013355. [PubMed] [Google Scholar]Chiumello D., Marino A., Cressoni M., Mietto C., Berto V., Gallazzi E., et al.  (2013). Pleural effusion in patients with acute lung injury: a CT scan study.\nCrit. Care Med.\n41\n935\u2013944. 10.1097/ccm.0b013e318275892c\n [PubMed] [CrossRef] [Google Scholar]Cornejo R. A., Diaz J. C., Tobar E. A., Bruhn A. R., Ramos C. A., Gonzalez R. A., et al.  (2013). Effects of prone positioning on lung protection in patients with acute respiratory distress syndrome.\nAm. J. Respir. Crit. Care Med.\n188\n440\u2013448. [PubMed] [Google Scholar]Cressoni M., Gallazzi E., Chiurazzi C., Marino A., Brioni M., Menga F., et al.  (2013). Limits of normality of quantitative thoracic CT analysis.\nCrit. Care\n17:R93. 10.1186/cc12738\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]Currie G., Hawk K. E., Rohren E., Vial A., Klein R. (2019). Machine Learning and Deep Learning in Medical Imaging: intelligent Imaging.\nJ. Med. Imaging Radiat. Sci.\n50\n477\u2013487. 10.1016/j.jmir.2019.09.005\n [PubMed] [CrossRef] [Google Scholar]Dong X., Lei Y., Wang T., Thomas M., Tang L., Curran W. J., et al.  (2019). Automatic multiorgan segmentation in thorax ct images using u-Net-gan.\nMed. Phys.\n46\n2157\u20132168. 10.1002/mp.13458\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]Gattinoni L., Caironi P., Cressoni M., Chiumello D., Ranieri V. M., Quintel M., et al.  (2006). Lung recruitment in patients with the acute respiratory distress syndrome.\nN. Engl. J. Med.\n354\n1775\u20131786. [PubMed] [Google Scholar]Gattinoni L., Pelosi P., Vitale G., Pesenti A., D\u2019Andrea L., Mascheroni D. (1991). Body position changes redistribute lung computed-tomographic density in patients with acute respiratory failure.\nAnesthesiology\n74\n15\u201323. 10.1097/00000542-199101000-00004\n [PubMed] [CrossRef] [Google Scholar]Gattinoni L., Pesenti A. (2005). The concept of \u201cbaby lung\u201d.\nIntensive Care Med.\n31\n776\u2013784. 10.1007/s00134-005-2627-z\n [PubMed] [CrossRef] [Google Scholar]Gattinoni L., Pesenti A., Avalli L., Rossi F., Bombino M. (1987). Pressure-volume curve of total respiratory system in acute respiratory failure. Computed tomographic scan study.\nA. Rev. Respir. Dis.\n136\n730\u2013736. 10.1164/ajrccm/136.3.730\n [PubMed] [CrossRef] [Google Scholar]Gattinoni L., Presenti A., Torresin A., Baglioni S., Rivolta M., Rossi F., et al.  (1986). Adult respiratory distress syndrome profiles by computed tomography.\nJ. Thorac. Imaging\n1\n25\u201330. [PubMed] [Google Scholar]Gerard S. E., Herrmann J., Kaczka D. W., Musch G., Fernandez-Bustamante A., Reinhardt J. M. (2020). Multi-Resolution convolutional neural Networks for fully automated segmentation of acutely injured lungs in multiple species,\u201d.\nMed. Image Anal.\n60:101592. 10.1016/j.media.2019.101592\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]Gerard S. E., Herrmann J., Xin Y., Martin K. T., Rezoagli E., Ippolito D., et al.  (2021). CT image segmentation for inflamed and fibrotic lungs using a multi-resolution convolutional neural Network.\nSci. Rep.\n11:1455. 10.1038/s41598-020-80936-4\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]Ghosh S., Chaki A., Santosh K. C. (2021). Improved U-Net architecture with VGG-16 for brain tumor segmentation.\nPhys. Eng. Sci. Med.\n10.1007/s13246-021-01019-w\n[Epub ahead of print].  [PubMed] [CrossRef] [Google Scholar]Glorot X., Bengio Y. (2010). Understanding the difficulty of training deep feedforward neural Networks.\nProc. Thirteenth Int. Conf. Artif. Intell. Stat.\n9\n249\u2013256. [Google Scholar]Goodfellow I., Bengio Y., Courville A. (2018). Deep Learning.\nCambridge, Massachusetts: MIT Press. [Google Scholar]Guimar\u00e3es P., Keller A., Fehlmann T., Lammert F., Casper M. (2021). Deep-learning based detection of eosinophilic esophagitis.\nEndoscopy\n10.1055/a-1520-8116\n[Epub ahead of print].  [PubMed] [CrossRef] [Google Scholar]Hinton G. E., Osindero S., Teh Y.-W. (2006). A fast learning algorithm for deep belief Nets.\nNeural Comp.\n18\n1527\u20131554. 10.1162/neco.2006.18.7.1527\n [PubMed] [CrossRef] [Google Scholar]Hofmanninger J., Prayer F., Pan J., R\u00f6hrich S., Prosch H., Langs G. (2020). Automatic lung segmentation in routine imaging is primarily a data diversity problem, not a methodology problem.\nEur. Radiol. Exp.\n4:50. 10.1186/s41747-020-00173-2\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]Hojin K., Jinhong J., Jieun K., Byungchul C., Jungwon K., Jeong Y. J., et al.  (2020). Abdominal multi-organ auto-segmentation using 3D-patch-based deep convolutional neural Network.\nSci. Rep.\n10:6204. 10.1038/s41598-020-63285-0\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]Hu X., Luo W., Hu J., Guo S., Huang W., Scott M. R., et al.  (2020). Brain SegNet: 3D local refinement Network for brain lesion segmentation.\nBMC Med. Imaging\n20:17. 10.1186/s12880-020-0409-2\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]Ioffe S., Szegedy C. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Available online at: https://arxiv.org/abs/1502.03167\n(accessed on 17 Feb 2020). [Google Scholar]Jalali Y., Fateh M., Rezvani M., Abolghasemi V., Anisi M. H. (2021). ResBCDU-Net: a Deep Learning Framework for Lung CT Image Segmentation.\nSensors\n21:268. 10.3390/s21010268\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]Jeong Y., Rachmadi M. F., Vald\u00e9s-Hern\u00e1ndez M. D. C., Komura T. (2019). Dilated Saliency U-Net for White Matter Hyperintensities Segmentation Using Irregularity Age Map.\nFront. Aging Neurosci.\n11:150. 10.3389/fnagi.2019.00150\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]J\u00fcnger S. T., Hoyer U. C. I., Schaufler D., Laukamp K. R., Goertz L., Thiele F., et al.  (2021). Fully Automated MR Detection and Segmentation of Brain Metastases in Non-small Cell Lung Cancer Using Deep Learning.\nJ. Magn. Reson. Imaging\n10.1002/jmri.27741\n[Epub ahead of print].  [PubMed] [CrossRef] [Google Scholar]Klapsing P., Herrmann P., Quintel M., Moerer O. (2017). Automatic quantitative computed tomography segmentation and analysis of aerated lung volumes in acute respiratory distress syndrome-A comparative diagnostic study.\nJ. Crit. Care\n42\n184\u2013191. 10.1016/j.jcrc.2016.11.001\n [PubMed] [CrossRef] [Google Scholar]Kumar Singh V., Abdel-Nasser M., Pandey N., Puig D. (2021). LungINFseg: segmenting COVID-19 Infected Regions in Lung CT Images Based on a Receptive-Field-Aware Deep Learning Framework.\nDiagnostics\n11:158. 10.3390/diagnostics11020158\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]LeCun Y., Bengio Y., Hinton G. (2015). Deep learning.\nNature\n521\n436\u2013444. [PubMed] [Google Scholar]Lei Y., Fu Y., Roper J., Higgins K., Bradley J. D., Curran W. J., et al.  (2021). Echocardiographic image multi-structure segmentation using Cardiac-SegNet.\nMed. Phys.\n48\n2426\u20132437. 10.1002/mp.14818\n [PubMed] [CrossRef] [Google Scholar]Litjens G., Kooi T., Bejnordi B. E., Setio A. A. A., Ciompi F., Ghafoorian M., et al.  (2017). A survey on deep learning in medical image analysis.\nMed. Image Anal.\n42\n60\u201388. [PubMed] [Google Scholar]Maas A. L., Hannun A. Y., Ng A. Y. (2013). Rectifier Nonlinearities Improve Neural Network Acoustic Models.\nProc. ICML\n30:3. [Google Scholar]Ngene (2021). DeepLTK Deep Learning Toolkit for LabView. Available online at: https://www.ngene.co/deep-learning-toolkit-for-labview\n(accessed August 17, 2021). [Google Scholar]Nowozin S. (2014). \u201cOptimal Decisions from Probabilistic Models: The Intersection-over-Union Case Published 2014 Mathematics, Computer Science,\u201d in\nIEEE Conference on Computer Vision and Pattern Recognition (Piscataway: IEEE; ), 10.1109/CVPR.2014.77 [CrossRef] [Google Scholar]Park J., Yun J., Kim N., Park B., Cho Y., Park H. J., et al.  (2020). Fully Automated Lung Lobe Segmentation in Volumetric Chest CT with 3D U-Net: validation with Intra- and Extra-Datasets.\nJ. Digit. Imaging\n33\n221\u2013230. 10.1007/s10278-019-00223-1\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]Pelosi P., D\u2019Andrea L., Vitale G., Pesenti A., Gattinoni L. (1994). Vertical gradient of regional lung inflation in adult respiratory distress syndrome.\nAm. J. Respir. Crit. Care Med.\n149\n8\u201313. 10.1164/ajrccm.149.1.8111603\n [PubMed] [CrossRef] [Google Scholar]Pelosi P., Tubiolo D., Mascheroni D., Vicardi P., Crotti S., Valenza F., et al.  (1998). Effects of the prone position on respiratory mechanics and gas exchange during acute lung injury.\nAm. J. Respir. Crit. Care Med.\n157\n387\u2013393. 10.1164/ajrccm.157.2.97-04023\n [PubMed] [CrossRef] [Google Scholar]Qiblawey Y., Tahir A., Chowdhury M. E. H., Khandakar A., Kiranyaz S., Rahman T., et al.  (2021). Detection and Severity Classification of COVID-19 in CT Images Using Deep Learning.\nDiagnostics\n11:893. 10.3390/diagnostics11050893\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]Ronneberger O., Fischer P., Brox T. (2015). \u201cU-Net: convolutional Networks for biomedical image segmentation,\u201d in\nInMedical Image Computing and Computer-Assisted Intervention \u2013 MICCAI 2015. MICCAI 2015. Lecture Notes in Computer Science, eds\nNavab N., Hornegger J., Wells W., Frangi A. (Cham: Springer; ), 234\u2013241. [Google Scholar]Saood A., Hatem I. (2021). COVID-19 lung CT image segmentation using deep learning methods: U-Net versus SegNet.\nBMC Med. Imaging\n21:19. 10.1186/s12880-020-00529-5\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]Schwartz J. T., Cho B. H., Tang P., Schefflein J., Arvind V., Kim J. S., et al.  (2021). Deep Learning Automates Measurement of Spinopelvic Parameters on Lateral Lumbar Radiographs.\nSpine\n46\nE671\u2013E678. 10.1097/BRS.0000000000003830\n [PubMed] [CrossRef] [Google Scholar]Seo H., Badiei Khuzani M., Vasudevan V., Huang C., Ren H., Xiao R., et al.  (2020a). Machine learning techniques for biomedical image segmentation: an overview of technical aspects and introduction to state-of-art applications.\nMed. Phys.\n47\ne148\u2013e167. 10.1002/mp.13649\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]Seo H., Huang C., Bassenne M., Xiao R., Xing L. (2020b). Modified U-Net (mU-Net) With Incorporation of Object-Dependent High Level Features for Improved Liver and Liver-Tumor Segmentation in CT Images.\nIEEE Trans. Med. Imaging\n39\n1316\u20131325. 10.1109/TMI.2019.2948320\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]Skourt B. A., El Hassani A., Majda A. (2018). Lung CT Image Segmentation Using Deep Neural Networks.\nProcedia Comput. Sci.\n127\n109\u2013113. 10.1016/j.procs.2018.01.104 [CrossRef] [Google Scholar]Sravani K. (2019). Medical Image Segmentation Using Deep Learning Using SegNet.\nIOSR J. Eng.\n09\n28\u201335. [Google Scholar]Srivastava N., Hinton G., Krizhevsky A., Sutskever I., Salakhutdinov R. (2014). Dropout: a simple way to prevent neural Networks from overfitting.\nJ. Mach. Learn. Res.\n15\n1929\u20131958. [Google Scholar]Su\u0142ot D., Alonso-Caneiro D., Ksieniewicz P., Krzyzanowska-Berkowska P., Iskander D. R. (2021). Glaucoma classification based on scanning laser ophthalmoscopic images using a deep learning ensemble method.\nPLoS One\n16:e0252339. 10.1371/journal.pone.0252339\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]Suzuki K. (2017). Overview of deep learning in medical imaging.\nRadiol. Phys. Technol.\n10\n257\u2013273. 10.1007/s12194-017-0406-5\n [PubMed] [CrossRef] [Google Scholar]Umapathy L., Unger W., Shareef F., Arif H., Mart\u00edn D., Altbach M., et al.  (2020). A Cascaded Residual UNET for Fully Automated Segmentation of Prostate and Peripheral Zone in T2-weighted 3D Fast Spin Echo Images.\nArXiv Available Online at: https://arxiv.org/ftp/arxiv/papers/2012/2012.13501.pdf\n(accessed August 17, 2021). [Google Scholar]Wang C., Shao J., Lv J., Cao Y., Zhu C., Li J., et al.  (2021). Deep learning for predicting subtype classification and survival of lung adenocarcinoma on computed tomography.\nTransl. Oncol.\n14:101141. 10.1016/j.tranon.2021.101141\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]Wang Z., Zou Y., Liu P. X. (2021). Hybrid dilation and attention residual U-Net for medical image segmentation.\nComput. Biol. Med.\n134:104449. 10.1016/j.compbiomed.2021.104449\n [PubMed] [CrossRef] [Google Scholar]Yan Y., Zhang D. (2021). Multi-scale U-like Network with attention mechanism for automatic pancreas segmentation.\nPLoS One\n16:e0252287. 10.1371/journal.pone.0252287\n [PMC free article] [PubMed] [CrossRef] [Google Scholar]Yasaka K., Akai H., Kunimatsu A., Kiryu S., Abe O. (2018). Deep learning with convolutional neural Network in radiology.\nJpn. J. Radiol.\n36\n257\u2013272. 10.1007/s11604-018-0726-3\n [PubMed] [CrossRef] [Google Scholar]Yi P. H., Wei J., Kim T. K., Shin J., Sair H. I., Hui F. K., et al.  (2021). Radiology \u201cforensics\u201d: determination of age and sex from chest radiographs using deep learning.\nEmerg. Radiol.\n10.1007/s10140-021-01953-y\n[Epub ahead of print].  [PubMed] [CrossRef] [Google Scholar]Zhou T., Canu S., Ruan S. (2020). Automatic COVID-19 CT segmentation using U-Net integrated spatial and channel attention mechanism.\nInt. J. Imaging Syst. Technol.\n10.1002/ima.22527\n[Epub ahead of print].  [PMC free article] [PubMed] [CrossRef] [Google Scholar]Zhou Z., Siddiquee M. M. R., Tajbakhsh N., Liang J. (2018). UNet++: a Nested U-Net Architecture for Medical Image Segmentation.\nDeep Learn. Med. Image Anal. Multimodal Learn. Clin. Decis. Support\n2018\n3\u201311. 10.1007/978-3-030-00889-5_1 [PMC free article] [PubMed] [CrossRef] [Google Scholar]"}